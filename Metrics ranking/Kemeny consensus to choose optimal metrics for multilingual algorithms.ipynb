{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6959a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy .stats import kendalltau\n",
    "from scipy .stats import rankdata\n",
    "from itertools import permutations\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9010e26a",
   "metadata": {},
   "source": [
    "# Aggregating the results of different metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531ae0c",
   "metadata": {},
   "source": [
    "Some translation algorithms are multilingual by design. We focus on a specific case in which we are looking to evaluate the quality of traductions from a language to another, where these languages are not known. Knowing that the quality of metrics for NLG algorithms (in the sense of a correlation with human evaluation) depends on the languages that are translated, we would like to find a ranking of metrics that is robust to the different languages considered. \n",
    "We reverse the classical point of view of using fixed metrics to evaluate algorithms on different tasks. We can then use Kemeny consensus to evaluate the performance of metrics  on fixed tasks, with fixed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe78b37",
   "metadata": {},
   "source": [
    "## Kemeny consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610481f9",
   "metadata": {},
   "source": [
    "We use fixed datasets that are annotated and we apply different metrics to these datasets. We can the rank the metrics on these different datasets (same data for all metrics) according to the correlation of these metrics with human evaluation. Then we need to code a Kemeny ranking consensus to order the metrics according to their performances on different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f5f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_negative(n) :\n",
    "    if n < 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "#Construction of a Kendall distance function\n",
    "def Kendall_distance(l1, l2):\n",
    "    if len(l1) != len(l2):\n",
    "        raise ValueError(\"Les permutations n'ont pas la mÃªme longueur\")\n",
    "    result = 0\n",
    "    for i in range (len(l1)):\n",
    "        for j in range (len(l1)):\n",
    "            result += is_negative((l1[i]-l1[j])*(l2[i]-l2[j]))\n",
    "    normalization = len(l1)*(len(l1)-1)\n",
    "    #normalization is made make the distance be in [0,1]\n",
    "    return result/normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeeb52b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(Kendall_distance([0, 1, 2], [0, 1, 2]))\n",
    "print(Kendall_distance([0, 1, 2], [2, 1, 0]))\n",
    "print(Kendall_distance([0, 1, 2], [1, 0, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d44a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When having a matrix M of different rankings of metrics on datasets based on their correlations with human evaluation\n",
    "def Kemeny_consensus_step1(M, perm):\n",
    "    #M is an array in which M[i,j] is the ranking of metrics j on the dataset i\n",
    "    result = 0\n",
    "    for i in range (M.shape[0]):\n",
    "        result += Kendall_distance (perm, M[i])\n",
    "    return result\n",
    "\n",
    "def Kemeny_consensus(M) : \n",
    "    L = []\n",
    "    dist = Kemeny_consensus_step1(M, M[0])\n",
    "    perm = permutations(M[0])\n",
    "    for permutation in list(perm) : \n",
    "        dist_travail = Kemeny_consensus_step1(M, permutation)\n",
    "        if dist_travail < dist:\n",
    "            L=[]\n",
    "            L += [np.array(permutation)]\n",
    "            dist = dist_travail\n",
    "        elif dist == dist_travail:\n",
    "            L += [np.array(permutation)]\n",
    "    return L, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d22d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.333333333333333\n",
      "1.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "M = np.array([[3, 2, 1, 0], [1, 2, 3, 0], [1, 3, 2, 0], [0, 1, 3, 2]])\n",
    "perm1 = [0, 1, 2, 3]\n",
    "print (Kemeny_consensus_step1(M, perm1))\n",
    "perm2 = [3, 2, 1, 0]\n",
    "print (Kemeny_consensus_step1(M, perm2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc09d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1, 3, 2, 0]), array([1, 2, 3, 0])], 1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kemeny_consensus(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe070ef",
   "metadata": {},
   "source": [
    "## Improvement of Kemeny consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c69cf74",
   "metadata": {},
   "source": [
    "All the rankings, given the different metrics have exactly the same weights. This might be a problem if we consider that some of the metrics are more relevant than others, or if some metrics are usually very close. So we can improve the Kemeny consensus by giving a higher weight to metrics that are more relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b994fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weighted_Gameny_consensus_step1(M, perm, Weights):\n",
    "    #M is an array in which M[i,j] is the ranking of algorithm j at test i\n",
    "    #Weights[i] is the weight associated to test i\n",
    "    result = 0\n",
    "    for i in range (M.shape[0]):\n",
    "        result += Kendall_distance (perm, M[i]) * Weights[i]\n",
    "    return result\n",
    "\n",
    "def Weighted_Kemeny_consensus(M, Weights) : \n",
    "    L = []\n",
    "    dist = Weighted_Gameny_consensus_step1(M, M[0], Weights)\n",
    "    perm = permutations(M[0])\n",
    "    for permutation in list(perm) : \n",
    "        dist_travail = Weighted_Gameny_consensus_step1(M, permutation, Weights)\n",
    "        if dist_travail < dist:\n",
    "            L=[]\n",
    "            L += [np.array(permutation)]\n",
    "            dist = dist_travail\n",
    "        elif dist == dist_travail:\n",
    "            L += [np.array(permutation)]\n",
    "    return L, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0070ec32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([3, 2, 1, 0])], 0.16666666666666669)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weights=[0.7, 0.1, 0.1, 0.1]\n",
    "Weighted_Kemeny_consensus(M, Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b80eb781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the correlations we can deduce \n",
    "def using_indexed_assignment(x):\n",
    "    result = np.empty(len(x), dtype=int)\n",
    "    x=-x\n",
    "    temp = x.argsort()\n",
    "    result[temp] = np.arange(len(x))\n",
    "    return result\n",
    "\n",
    "def get_ranking(df):\n",
    "    ranking = df\n",
    "    for col in df.columns:\n",
    "        ranking[col] = using_indexed_assignment(df[col])\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0f230",
   "metadata": {},
   "source": [
    "# Testing our methods on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b8b09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Kendall_cs_en = pd.read_csv('data\\corr_kendall_2016_cs-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Pearson_cs_en = pd.read_csv('data\\corr_pearson_2016_cs-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Spearman_cs_en = pd.read_csv('data\\corr_spearman_2016_cs-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Kendall_de_en = pd.read_csv('data\\corr_kendall_2016_de-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Pearson_de_en = pd.read_csv('data\\corr_pearson_2016_de-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Spearman_de_en = pd.read_csv('data\\corr_spearman_2016_de-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Kendall_fi_en = pd.read_csv('data\\corr_kendall_2016_fi-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Pearson_fi_en = pd.read_csv('data\\corr_pearson_2016_fi-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Spearman_fi_en = pd.read_csv('data\\corr_spearman_2016_fi-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Kendall_ro_en = pd.read_csv('data\\corr_kendall_2016_ro-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Pearson_ro_en = pd.read_csv('data\\corr_pearson_2016_ro-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Spearman_ro_en = pd.read_csv('data\\corr_spearman_2016_ro-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Kendall_ru_en = pd.read_csv('data\\corr_kendall_2016_ru-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Pearson_ru_en = pd.read_csv('data\\corr_pearson_2016_ru-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)\n",
    "df_Spearman_ru_en = pd.read_csv('data\\corr_spearman_2016_ru-en.csv').set_index('Unnamed: 0').abs().drop(['depth', 'rouge2', 'human_scores'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b2bfe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_Kendall_cs = get_ranking(df_Kendall_cs_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Kendall_cs'})).transpose()\n",
    "ranking_Pearson_cs = get_ranking(df_Pearson_cs_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Pearson_cs'})).transpose()\n",
    "ranking_Spearman_cs = get_ranking(df_Spearman_cs_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Spearman_cs'})).transpose()\n",
    "ranking_Kendall_de = get_ranking(df_Kendall_de_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Kendall_de'})).transpose()\n",
    "ranking_Pearson_de = get_ranking(df_Pearson_de_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Pearson_de'})).transpose()\n",
    "ranking_Spearman_de = get_ranking(df_Spearman_de_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Spearman_de'})).transpose()\n",
    "ranking_Kendall_fi = get_ranking(df_Kendall_fi_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Kendall_fi'})).transpose()\n",
    "ranking_Pearson_fi = get_ranking(df_Pearson_fi_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Pearson_fi'})).transpose()\n",
    "ranking_Spearman_fi = get_ranking(df_Spearman_fi_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Spearman_fi'})).transpose()\n",
    "ranking_Kendall_ro = get_ranking(df_Kendall_ro_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Kendall_ro'})).transpose()\n",
    "ranking_Pearson_ro = get_ranking(df_Pearson_ro_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Pearson_ro'})).transpose()\n",
    "ranking_Spearman_ro = get_ranking(df_Spearman_ro_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Spearman_ro'})).transpose()\n",
    "ranking_Kendall_ru = get_ranking(df_Kendall_ru_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Kendall_ru'})).transpose()\n",
    "ranking_Pearson_ru = get_ranking(df_Pearson_ru_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Pearson_ru'})).transpose()\n",
    "ranking_Spearman_ru = get_ranking(df_Spearman_ru_en.loc[:,['human_scores']].rename(columns= {'human_scores' : 'Spearman_ru'})).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192ce4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bary</th>\n",
       "      <th>bertscore</th>\n",
       "      <th>bleu</th>\n",
       "      <th>chrf</th>\n",
       "      <th>meteor</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>sacrebleu</th>\n",
       "      <th>ter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spearman_ro</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Unnamed: 0   bary  bertscore  bleu  chrf  meteor  rouge1  rougeL  sacrebleu  \\\n",
       "Spearman_ro     1          0     8     2       3       5       4          6   \n",
       "\n",
       "Unnamed: 0   ter  \n",
       "Spearman_ro    7  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_Kendall_ro\n",
    "ranking_Pearson_ro\n",
    "ranking_Spearman_ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8219049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([ranking_Kendall_cs, ranking_Pearson_cs, ranking_Spearman_cs,\n",
    "#           ranking_Kendall_de, ranking_Pearson_de, ranking_Spearman_de,\n",
    "#           ranking_Kendall_fi, ranking_Pearson_fi, ranking_Spearman_fi,\n",
    "#           ranking_Kendall_ro, ranking_Pearson_ro, ranking_Spearman_ro,\n",
    "#           ranking_Kendall_ru, ranking_Pearson_ru, ranking_Spearman_ru], axis = 0)\n",
    "\n",
    "Kendall_scores = pd.concat([ranking_Kendall_cs,\n",
    "           ranking_Kendall_de,\n",
    "           ranking_Kendall_fi,\n",
    "           ranking_Kendall_ro,\n",
    "           ranking_Kendall_ru], axis = 0)\n",
    "\n",
    "Pearson_scores = pd.concat([ranking_Pearson_cs,\n",
    "           ranking_Pearson_de,\n",
    "           ranking_Pearson_fi,\n",
    "           ranking_Pearson_ro,\n",
    "           ranking_Pearson_ru,], axis = 0)\n",
    "\n",
    "\n",
    "Spearman_scores = pd.concat([ranking_Spearman_cs,\n",
    "           ranking_Spearman_de,\n",
    "           ranking_Spearman_fi,\n",
    "           ranking_Spearman_ro,\n",
    "           ranking_Spearman_ru,], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eccf8b",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3a58319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1, 0, 8, 4, 3, 5, 2, 7, 6])], 0.2222222222222222)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kemeny_consensus(Spearman_scores.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "025b8fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1, 0, 8, 3, 4, 5, 2, 6, 7])], 0.2222222222222222)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kemeny_consensus(Pearson_scores.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0014bc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1, 0, 8, 4, 3, 5, 2, 7, 6])], 0.3055555555555556)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kemeny_consensus(Kendall_scores.to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
